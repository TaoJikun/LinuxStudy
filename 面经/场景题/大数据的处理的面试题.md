[TOC]

<https://blog.csdn.net/v_JULY_v/article/details/7382693?utm_medium=distribute.pc_relevant.none-task-blog-2%7Edefault%7EBlogCommendFromMachineLearnPai2%7Edefault-13.essearch_pc_relevant&depth_1-utm_source=distribute.pc_relevant.none-task-blog-2%7Edefault%7EBlogCommendFromMachineLearnPai2%7Edefault-13.essearch_pc_relevant>
## 批量导入数据 查询数据 高g文件搜索

## 海量日志的数据，找到ip访问最多的那个

ip地址32位，2的32次方是好几个G，比较大，不能直接比较，可以按照分治法来解决，将ip分成1024份，分的方法有多种，但是hash最好，按照ip后十位来分也可以，分成1024个文件，这样的话每个就是4M，可以方便的加载到内存里边。hashMap，存储最大的，没必要用priorityHashMap，因为ip的变动太多，会消耗太多资源。

## 一千万个查询记录，去除重复的有300万个，求前十高频词

假设目前有一千万个记录（这些查询串的重复度比较高，虽然总数是1千万，但如果除去重复后，不超过3百万个。一个查询串的重复度越高，说明查询它的用户越多，也就是越热门。），请你统计最热门的10个查询串，要求使用的内存不能超过1G。

 第一步、先对这批海量数据预处理，在O（N）的时间内用Hash表完成统计（之前写成了排序，特此订正。July、2011.04.27）；
    第二步、借助堆这个数据结构，找出Top K，时间复杂度为N‘logK。
        即，借助堆结构，我们可以在log量级的时间内查找和调整/移动。因此，维护一个K(该题目中是10)大小的小根堆，然后遍历300万的Query，分别和根元素进行对比所以，我们最终的时间复杂度是：O（N） + N'*O（logK），（N为1000万，N’为300万）。ok，更多，详情，请参考原文。
注意在堆的排序 由于是求前K大，所以小根堆是符合要求的 但是需要重写Comparator。 

## 有一个1G大小的一个文件，里面每一行是一个词，词的大小不超过16字节，内存限制大小是1M。返回频数最高的100个词。

 方案：顺序读文件中，对于每个词x，取hash(x)%5000，然后按照该值的对应存到5000个小文件（记为x0,x1,...x4999）中。这样每个文件大概是200k左右。

    如果其中的有的文件超过了1M大小，还可以按照类似的方法继续往下分，直到分解得到的小文件的大小都不超过1M。
    对每个小文件，统计每个文件中出现的词以及相应的频率（可以采用trie树/hash_map等），并取出出现频率最大的100个词（可以用含100个结点的最小堆），并把100个词及相应的频率存入文件，这样又得到了5000个文件。下一步就是把这5000个文件进行归并（类似与归并排序）的过程了。
##  **给定a、b两个文件，各存放50亿个url，每个url各占64字节，内存限制是4G，让你找出a、b文件共同的url？**

首先将a文件和b文件中的url，按照hash来分成 1000 个文件，为什么要分成1000，只是1000保证没问题。

按照a1,a2。。。b1...b2这样的形式来存储，这样的话 a1 在b1 中寻找，肯定跑不了

## 在2.5亿个整数里找到一个不重复的整数。**注，内存不足以容纳这2.5亿个整数**

两种方法 ，第一种方法最好：

- 首先是用bitmap存储数字状态，遍历这个2.5亿的数字，初出现多次就10，0次就00，1次就01，11我们不赋值。这样需要的空间大小是 0.25 * 2^30*2 大小是0.5G，这样再便利一次，找到01的退出就行了
- 第二种也是分治法，将2.5亿的数字，分成1000 组，再将1000组求出来。

## **腾讯面试题：给40亿个不重复的unsigned int的整数，没排过序的，然后再给一个数，如何快速判断这个数是否在那40亿个数当中？**

- 40亿个unsigned int，其实就是基本上大部分整个无符号整数【unsigned int】的范围，所以我们可以使用bit位来代替。申请512M的内存，一个bit位代表一个unsigned int值。读入40亿个数，设置相应的bit位，读入要查询的数，查看相应bit位是否为1，为1表示存在，为0表示不存在。

-   然后将这40亿个数分成两类:
        1.最高位为0
        2.最高位为1
      并将这两类分别写入到两个文件中，其中一个文件中数的个数<=20亿，而另一个>=20亿（这相当于折半了）；
  与要查找的数的最高位比较并接着进入相应的文件再查找

      再然后把这个文件为又分成两类:
        1.次最高位为0
        2.次最高位为1
      
      并将这两类分别写入到两个文件中，其中一个文件中数的个数<=10亿，而另一个>=10亿（这相当于折半了）；
      与要查找的数的次最高位比较并接着进入相应的文件再查找。
      .......
      以此类推，就可以找到了,而且时间复杂度为O(logn)，方案2完。
  ————————————————
  版权声明：本文为CSDN博主「v_JULY_v」的原创文章，遵循CC 4.0 BY-SA版权协议，转载请附上原文出处链接及本声明。
  原文链接：https://blog.csdn.net/v_JULY_v/article/details/6279498

- 还有一个方法 位图法

## **怎么在海量数据中找出重复次数最多的一个？**

海量数据，所以不能直接对比，但是有要求重复数据，所以我们既要分开，又要按照重复的数据都放在同一组这样的顺序进行。所以怎么分组，直接hash %10000 这样来对比 在每个分组里边找重复最多的。

## **上千万或上亿数据（有重复），统计其中出现次数最多的钱N个数据。**

上千万的数据或者上亿的数据，其最终的大小可能几G，如果没有限制内存大小，那么可以使用hashMap+priorityQueue直接进行

## **一个文本文件，大约有一万行，每行一个词，要求统计出其中最频繁出现的前10个词，请给出思想，给出时间复杂度分析。**

分而治之的想法 不再赘述

## 十G个数字，求中位数。**要求找出中位数。内存限制为2G。**

十G的话，如果是unsigned int，类型，我们根据统计数字的思想，10亿的话，那么中位数就是5G那里，所以我们要统计个数，并且需要从大大小统计个数。假设是unsigned int类型的数字，那么总共是4G个数字，所以int类型不够，我们需要一个longlong类型的用来计算个数【防止出现所有数字都一样的情况】

然后因为我们需要分组，但是又为了最大空间的利用2G内存，longlong类型的占比为64位也就是8B，所以，2GB/8B = 256M，我们可以分成256M组，每一组放这个范围内的数字。

然后再扫描一遍，将这个范围的数字找出来，然后根据计数等，求出中位数。



## 批量向Mysql中导入数据 应该使用什么方法





